\BOOKMARK [1][-]{section.1}{Einleitung und Motivation}{}% 1
\BOOKMARK [2][-]{subsection.1.1}{Ziel der Arbeit}{section.1}% 2
\BOOKMARK [2][-]{subsection.1.2}{Nichtparametrische Regression}{section.1}% 3
\BOOKMARK [2][-]{subsection.1.3}{Konvergenzrate}{section.1}% 4
\BOOKMARK [2][-]{subsection.1.4}{Notationen}{section.1}% 5
\BOOKMARK [1][-]{section.2}{Beschreibung des Modells}{}% 6
\BOOKMARK [2][-]{subsection.2.1}{Motivation und Definition eines neuronalen Netzwerkes}{section.2}% 7
\BOOKMARK [2][-]{subsection.2.2}{Rahmenbedingungen des Modells}{section.2}% 8
\BOOKMARK [3][-]{subsubsection.2.2.1}{Aktivierungsfunktion}{subsection.2.2}% 9
\BOOKMARK [3][-]{subsubsection.2.2.2}{Netzwerkparameter}{subsection.2.2}% 10
\BOOKMARK [3][-]{subsubsection.2.2.3}{D\374nnbesetzte Parameter}{subsection.2.2}% 11
\BOOKMARK [3][-]{subsubsection.2.2.4}{Hierarchische Komposition der Regressionsfunktion}{subsection.2.2}% 12
\BOOKMARK [2][-]{subsection.2.3}{Glattheit einer kompositionalen Funktion}{section.2}% 13
\BOOKMARK [2][-]{subsection.2.4}{Empirisches Risiko}{section.2}% 14
\BOOKMARK [1][-]{section.3}{Hauptresultat}{}% 15
\BOOKMARK [2][-]{subsection.3.1}{Obere Schranke des L2-Fehler}{section.3}% 16
\BOOKMARK [3][-]{subsubsection.3.1.1}{Folgerungen aus Theorem 1}{subsection.3.1}% 17
\BOOKMARK [2][-]{subsection.3.2}{Untere Schranke des L2-Fehler}{section.3}% 18
\BOOKMARK [2][-]{subsection.3.3}{Beweise}{section.3}% 19
\BOOKMARK [3][-]{subsubsection.3.3.1}{Einbettungseigenschaften einer Netzwerkfunktionsklasse}{subsection.3.3}% 20
\BOOKMARK [3][-]{subsubsection.3.3.2}{Approximationsqualit\344t neuronaler Netzwerke}{subsection.3.3}% 21
\BOOKMARK [3][-]{subsubsection.3.3.3}{Beweis zum Theorem 1}{subsection.3.3}% 22
\BOOKMARK [1][-]{section.4}{Beispiele}{}% 23
\BOOKMARK [2][-]{subsection.4.1}{Additive Modelle}{section.4}% 24
\BOOKMARK [2][-]{subsection.4.2}{Verallgemeinerte additive Modelle}{section.4}% 25
\BOOKMARK [1][-]{section.5}{Ausblick}{}% 26
